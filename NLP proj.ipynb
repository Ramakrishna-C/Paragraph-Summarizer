{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rouge\n",
    "import re\n",
    "import difflib\n",
    "from summa import summarizer\n",
    "from pdfminer.high_level import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_split_sentences(a, b):\n",
    "    match = difflib.SequenceMatcher(isjunk=None, a=a, b=b, autojunk=True)\n",
    "    lst_match = [block for block in match.get_matching_blocks() if block.size > 20]\n",
    "    \n",
    "    if len(lst_match) == 0:\n",
    "        lst_a, lst_b = nltk.sent_tokenize(a), nltk.sent_tokenize(b)\n",
    "    \n",
    "    else:\n",
    "        first_m, last_m = lst_match[0], lst_match[-1]\n",
    "\n",
    "        string = a[0 : first_m.a]\n",
    "        lst_a = [t for t in nltk.sent_tokenize(string)]\n",
    "        for n in range(len(lst_match)):\n",
    "            m = lst_match[n]\n",
    "            string = a[m.a : m.a+m.size]\n",
    "            lst_a.append(string)\n",
    "            if n+1 < len(lst_match):\n",
    "                next_m = lst_match[n+1]\n",
    "                string = a[m.a+m.size : next_m.a]\n",
    "                lst_a = lst_a + [t for t in nltk.sent_tokenize(string)]\n",
    "            else:\n",
    "                break\n",
    "        string = a[last_m.a+last_m.size :]\n",
    "        lst_a = lst_a + [t for t in nltk.sent_tokenize(string)]\n",
    "\n",
    "        string = b[0 : first_m.b]\n",
    "        lst_b = [t for t in nltk.sent_tokenize(string)]\n",
    "        for n in range(len(lst_match)):\n",
    "            m = lst_match[n]\n",
    "            string = b[m.b : m.b+m.size]\n",
    "            lst_b.append(string)\n",
    "            if n+1 < len(lst_match):\n",
    "                next_m = lst_match[n+1]\n",
    "                string = b[m.b+m.size : next_m.b]\n",
    "                lst_b = lst_b + [t for t in nltk.sent_tokenize(string)]\n",
    "            else:\n",
    "                break\n",
    "        string = b[last_m.b+last_m.size :]\n",
    "        lst_b = lst_b + [t for t in nltk.sent_tokenize(string)]\n",
    "    \n",
    "    return lst_a, lst_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_string_matching(a, b, both=True, sentences=True, titles=[]):\n",
    "    if sentences is True:\n",
    "        lst_a, lst_b = utils_split_sentences(a, b)\n",
    "    else:\n",
    "        lst_a, lst_b = a.split(), b.split()       \n",
    "    \n",
    "    ## highlight a\n",
    "    first_text = []\n",
    "    for i in lst_a:\n",
    "        if re.sub(r'[^\\w\\s]', '', i.lower()) in [re.sub(r'[^\\w\\s]', '', z.lower()) for z in lst_b]:\n",
    "            first_text.append('<span style=\"background-color:rgba(255,215,0,0.3);\">' + i + '</span>')\n",
    "        else:\n",
    "            first_text.append(i)\n",
    "    first_text = ' '.join(first_text)\n",
    "    \n",
    "    ## highlight b\n",
    "    second_text = []\n",
    "    if both is True:\n",
    "        for i in lst_b:\n",
    "            if re.sub(r'[^\\w\\s]', '', i.lower()) in [re.sub(r'[^\\w\\s]', '', z.lower()) for z in lst_a]:\n",
    "                second_text.append('<span style=\"background-color:rgba(255,215,0,0.3);\">' + i + '</span>')\n",
    "            else:\n",
    "                second_text.append(i)\n",
    "    else:\n",
    "        second_text.append(b) \n",
    "    second_text = ' '.join(second_text)\n",
    "    \n",
    "    ## concatenate\n",
    "    if len(titles) > 0:\n",
    "        first_text = \"<strong>\"+titles[0]+\"</strong><br>\"+first_text\n",
    "    if len(titles) > 1:\n",
    "        second_text = \"<strong>\"+titles[1]+\"</strong><br>\"+second_text\n",
    "    else:\n",
    "        second_text = \"---\"*65+\"<br><br>\"+second_text\n",
    "    final_text = first_text +'<br><br>'+ second_text\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summary(y_test, predicted):\n",
    "    rouge_score = rouge.Rouge()\n",
    "    scores = rouge_score.get_scores(y_test, predicted, avg=True)\n",
    "    score_1 = round(scores['rouge-1']['f'], 2)\n",
    "    score_2 = round(scores['rouge-2']['f'], 2)\n",
    "    score_L = round(scores['rouge-l']['f'], 2)\n",
    "    print(\"rouge1:\", score_1, \"| rouge2:\", score_2, \"| rougeL:\", score_2, \n",
    "          \"--> avg rouge:\", round(np.mean([score_1,score_2,score_L]), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BASELINE (Extractive Summarization : TextRank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank(corpus, ratio=0.2, words=20):\n",
    "    if isinstance(corpus, str):\n",
    "        corpus = [corpus]\n",
    "    \n",
    "    lst_summaries = [summarizer.summarize(txt, ratio=ratio, words=words) for txt in corpus]\n",
    "    return lst_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path):\n",
    "    text = extract_text(file_path)\n",
    "    return text\n",
    "\n",
    "# the headings should be capital\n",
    "def isTitle(st):\n",
    "    return st.upper() == st and not st.isnumeric()\n",
    "\n",
    "# returns the list of (title, para list)\n",
    "def parse(tex):\n",
    "    ret = []\n",
    "    tex += '\\n\\nEND'\n",
    "    curr_title = 'START'\n",
    "    curr_lis = []\n",
    "    \n",
    "    for i in re.split('\\n\\n', text):\n",
    "        if(isTitle(i) and not (curr_title.endswith('REFERENCES') or curr_title.endswith('BIBLIOGRAPHY'))):\n",
    "            ret.append((curr_title, curr_lis))\n",
    "            curr_title = i.strip()\n",
    "            curr_lis = []\n",
    "        else:\n",
    "            i = re.sub('\\[\\d+\\]', '', i)\n",
    "            if(not i.isnumeric()):\n",
    "                curr_lis.append(i.strip())\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text_from_pdf('Eye_Movement_Tracking_for_Computer_Vision_Syndrome_using_Deep_Learning_Techniques.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "START\n",
      "\n",
      "\n",
      "['Eye Movement Tracking for Computer Vision Syndrome using\\nDeep Learning Techniques', 'Manan Popat1, Divyan Goyal1, Vibhum Raj1, Nirmal Jayabalan2, and Chittaranjan Hota1', '1Department of Computer Science, BITS Pilani, Hyderabad Campus, India\\n2Department of Pharmacy, BITS Pilani, Hyderabad Campus, India\\n{f20200029, f20200042, f20200247, nirmalj, hota}@hyderabad.bits-pilani.ac.in', 'Abstract— Due to the increased usage of digital devices in\\ndaily life, particularly among children, symptoms such as\\ndrying of the eyes, eye strain, headaches, blurred vision, etc.,\\nhave become recurrent nowadays. Extensive use of computers\\nand smartphones may lead to a common eye-related condition\\nknown as Computer Vision Syndrome (CVS). It is often char-\\nacterized by a reduced blinking rate of the user. In this paper,\\nwe propose a deep neural network and computer vision-based\\nmachine learning model that entails training a Convolutional\\nNeural Network (CNN) to detect eye blinks, and monitoring\\nblink rates with a Long Short-Term Memory (LSTM) network.\\nThis model can be incorporated into smartphones and comput-\\ners in the form of background apps and may help prevent the\\nrisk of CVS or similar disorders. Inferences about the blink\\nrate and eye movement patterns have also been identified. Our\\nmodel is implemented using TensorFlow and Dlib libraries and\\nhas been trained on the Closed Eyes in the Wild (CEW) dataset.\\nThe network achieved an accuracy of 94.2% when trained on\\nnon-RGB images of eye patches and 91.4% on RGB facial\\nimages in real-time.']\n",
      "\n",
      "\n",
      "I. INTRODUCTION\n",
      "\n",
      "\n",
      "['Computer Vision Syndrome (CVS) is a term that\\nis\\ncommonly used nowadays to refer to a number of vision-\\nand eye-related difficulties caused by the increased usage\\nof computers, tablets, mobile phones, and other electronic\\ndevices . When not treated, this syndrome, also known\\nas “digital eye strain”, causes problems such as dry eyes,\\nastigmatism, far-sightedness, presbyopia, and others . The\\nuse of such digital gadgets has grown dramatically across\\nall age categories, especially among children. It is critical to\\nmonitor device usage to avoid potential eyesight problems.\\nDrying of the tear film is the primary cause of the fatigue,\\nburning sensation, and discomfort experienced due to CVS.\\nBlinking is an inbuilt mechanism that prevents tear film\\ndrying and ensures the ocular surface is lubricated .\\nHence, we are proposing to develop an intelligent application\\nto monitor the blinking rate, which is a vital sign of CVS.\\nExisting research in eye blink detection for CVS has\\nexplored various methods, but several limitations have been\\nidentified. One study  tackled edge extraction but strug-\\ngled with poor lighting conditions, highlighting the challenge\\nof capturing accurate data under uncontrolled lighting. An-\\nother work  focused on smartphone-based detection, find-\\ning that consistent lighting was essential for accurate results,\\nposing difficulties in outdoor environments. Moreover, a\\nmethod utilizing Harris corner detection  faced limitations', 'as it required setting distinct threshold values for each image,\\npotentially leading to noise-induced point detection when\\nusing low thresholds. These works underscore the need for\\nrobust illumination strategies, stable lighting conditions, and\\nimproved threshold determination methods to enhance the\\naccuracy and reliability of eye blink detection techniques for\\nCVS mitigation.', 'Bennett et al.  proposed a method for pupil seg-\\nmentation and gaze estimation in MR eye videos using a\\nfully convolutional neural network. However, their approach\\nmainly focused on static frame-by-frame training and did\\nnot address the real-time monitoring of eye blinking rates\\nin continuous live streams, which is crucial for CVS assess-\\nment. In another study, Jurczak et al.  utilized CNN for\\neye blink artifacts removal from EEG signals, showcasing\\nthe potential of CNNs for classification tasks. However, their\\nresearch focused on a different data type and domain, and\\nthe application to CVS eye blink detection requires a tailored\\nand robust approach.', 'Vishesh et al.  developed a blink detection model\\nusing MobileNetV2-based CNN to assess drowsiness levels\\nin drivers. Additionally, Gomaa et al.  proposed a CNN-\\nLSTM based deep learning approach in the same context.\\nThe current work lays emphasis on monitoring blink patterns\\nduring screen interaction, that often involves close-up images\\nof eyes to accurately assess blink quality and frequency.\\nDatasets in this domain consist of individuals using digital\\ndevices under various lighting conditions. In contrast, driver\\ndrowsiness detection necessitates real-time monitoring of\\nblinks from a distance, usually through vehicle-mounted\\ncameras. Techniques often involve more robust algorithms\\ndue to the need for accurate drowsiness prediction and timely\\nintervention. Consequently, while both contexts utilize blink\\ndetection, the strategies and datasets diverge to accommodate\\ndistinct goals and operational conditions.', 'In light of these existing studies, our research introduces\\na novel application of a combined approach utilizing a\\nconvolutional neural network (CNN) to extract eye features,\\nfollowed by a long short-term memory (LSTM) network to\\nmodel the temporal evolution of eye blinking in a continu-\\nous live stream input of individuals using a digital device.\\nThe general motivation behind using a CNN network for\\nclassification is their ability to learn and identify patterns\\nin visual data and automatically extract features from raw', 'images. Additionally, LSTM layers help model\\ntemporal\\ndependencies in sequential data and are commonly used\\nfor taking a continuous input stream (video frames) and\\nmonitoring the blink rate in real-time with accuracy and\\nrobustness.', 'Our methodology involved applying the proposed algo-\\nrithm to a diverse dataset of images of individual faces and\\neye patches collected from various sources. The output of\\nour model provides frame-level classifications, labelling each\\nframe as either “eye open” or “eye closed”. This research\\nmakes the following key contributions:', '(i) Introduction of a dual-stage network that combines\\neye features extracted via a CNN with their dynamic rep-\\nresentation modeled by an LSTM. This approach is tailored\\nspecifically for analyzing live stream videos of digital device\\nusers.', '(ii) Thorough experimental evaluation of model parame-\\nters, such as LSTM cell units and layers, and deploying the\\nmodel using OpenCV and Dlib for real-time monitoring.', '(iii) A survey of the various studies on the impact of usage\\nof electronic devices on the blink rate and eye movement\\npatterns.', 'The experimental results demonstrated the model’s robust-\\nness in detecting eye states, effectively handling considerable\\nvariations in visual quality and eye positions.']\n",
      "\n",
      "\n",
      "II. INFERENCES ABOUT IMPACT ON EYE BLINK\n",
      "RATE — A SURVEY\n",
      "\n",
      "\n",
      "['A lot of medical studies ,  have shown that the\\nact of blinking regularly is crucial for ensuring the dynamic\\nequilibrium between tear formation and tear on the ocular\\nsurface. Abnormalities in blinking causes the tear film to\\ndry up, which leads to irritation and pain in the ocular\\nsurface. Various studies have investigated the blink rate and\\nthe duration between blinks. According to these studies ,\\nthe average spontaneous blink rate ranges between 12 and\\n15 times per minute. In addition, the period between blinks\\nranges from 2.8 to 4 seconds or from 2 to 10 seconds.\\nThe differences in the results of previous studies may be\\nattributed to differences in the experimental conditions. Un-\\nder relaxed conditions, a mean blink rate of up to 22 per\\nminute has been reported .', 'In many studies , , the blink rate has been observed\\nto have reduced from 17 per minute during conversation\\nto about 6 per minute while reading. Moreover, discomfort\\nand tiredness in the eyes are frequently experienced when\\nengaging in near tasks like reading, especially when using\\nelectronic devices. There have been numerous studies con-\\nducted to investigate the connection between eye fatigue and\\nthe use of visual display terminals (VDTs) . People who\\nuse VDTs frequently complain of eye discomfort and fatigue.\\nAccording to a prior study , tired eyes was one of the\\nmost commonly reported symptoms among office workers\\n(40%), while 30% reported dry eyes and eye discomfort. On\\nan average, normal individuals and those with dry eyes expe-\\nrienced a 56% and 72% decrease in blink rates, respectively\\n.', 'The blink rate of dry eye subjects was significantly higher\\ncompared to the control group (according to a t-test with\\na p-value of 0.017), as identified by a survey . Both\\ngroups showed a significant decrease in blink rate during\\nthe game and letter tasks (according to a t-test with a p-\\nvalue of less than 0.04). Both groups exhibited instances of\\npartial blinks and rapid sequences of blinks, yet there was no\\nsignificant distinction in the amplitude of blinks between the\\ntwo groups. Tear film break-up was mostly inferior in normal\\nsubjects, while dry eye subjects showed more tear break-\\nups centrally and superiorly . The interaction between\\ntear break-up and blink behavior was complex and observed\\nin real-time video recordings. The researchers observed that\\ndry eye subjects experienced more intense ocular symptoms\\npost-testing compared to the control group. Both groups\\nshowed an increase in corneal staining post-testing, which\\nwas primarily inferior. A noticeable positive correlation was\\nobserved between the total symptom score and the proportion\\nof incomplete blinks, suggesting that a higher occurrence of\\nincomplete blinks was linked to more symptoms. In contrast,\\na significant inverse relationship was detected between the\\nblink score and symptoms, indicating that a higher blink rate\\nwas associated with fewer symptoms.']\n",
      "\n",
      "\n",
      "III. DATA\n",
      "\n",
      "\n",
      "['The dataset we have used for training the model is Closed\\nEyes in the Wild (CEW) . The dataset, prepared by\\nXiaoyang Tan, includes 2423 images. Of those, 1192 were\\nobtained from the Internet with their eyes closed, while\\nthe remaining 1231 were selected from the Labeled Face\\nin the Wild database with their eyes open. Eye patches\\nwere collected using a face detector and eye localization\\nsoftware based on the coarse face region and eye position.\\nThe cropped coarse faces were resized to 100x100 pixels,\\nand eye patches measuring 24x24 pixels were extracted from\\nthe eye position. The dataset consists of three versions: Raw\\nface images with background; Face images warped; and Eye\\npatches only (non-RGB). Fig. 1 shows some samples from\\nthe two kinds of images used in this study.', 'Pre-processing: Firstly, we load the images separately as\\nopen and closed (with labels 0 and 1, respectively) from the\\ndirectory and then combine them in a single list. The list is\\nthen shuffled and divided into features (image strings) and\\ntarget (binary labels), followed by test-train split (with test\\nsize = 0.2) and the required reshaping of images to 224x224.\\nThe target column is then converted into one-hot encoded\\nvectors.']\n",
      "\n",
      "\n",
      "IV. BINARY CLASSIFICATION MODEL\n",
      "\n",
      "\n",
      "['Transfer learning: Next, we build the model. We used\\nInception-v3 as the base model (Fig. 2), which serves as\\na feature extractor, and made the layers non-trainable (i.e.,\\ntheir weights are frozen during training). By doing so, the\\nmodel can learn to generalize better and hence requires\\nless training since the base model has already learned the\\ncomplex patterns in visual data (it is one of the state-of-\\nthe-art models for image classification). This approach also']\n",
      "\n",
      "\n",
      "TABLE I\n",
      "ARCHITECTURAL PARAMETERS FOR THE MODEL.\n",
      "\n",
      "\n",
      "['Parameter\\nBase model\\nPretraining dataset\\nReshape layer target shape\\nNumber of LSTM units\\nDepth of stacked LSTM\\nNumber of neurons in first dense layer\\nActivation function for first dense layer\\nDropout layer argument\\nActivation function for final dense (classification) layer', 'Value\\nInception-v3\\nImageNet\\n(1, 2048)\\n128\\n3\\n2048\\nReLU\\n0.3\\nSigmoid']\n",
      "\n",
      "\n",
      "TABLE II\n",
      "COMPILATION PARAMETERS FOR THE MODEL.\n",
      "\n",
      "\n",
      "['Parameter\\nModel optimizer\\nLoss function\\nMetric\\nNumber of epochs for training', 'Value\\nAdam\\nBinary cross-entropy\\nAccuracy\\n15', 'pre-processing (resizing to shape 224x224 and normalizing\\nthe image patch) before running it through the classifier and\\nreturning the processed output.', 'Dlib landmark detection: Dlib is an open-source library\\nused heavily for computer vision. It is one of the most\\nwidely used libraries providing state-of-the-art performance\\nin facial recognition. Dlib’s facial landmark detection model\\ncan identify the exact location of specific points on a face,\\nsuch as the corners of the eyes or the tip of the nose. We\\nused this model for the localization of eye and face patches\\n(which is further used to identify the ROI for the model).\\nBy calculating the extreme left and right ends of the eye\\nand applying a buffer, we get the location of individual eyes\\nand their corresponding image patches. These can then be\\ndirectly run through the model by using the function we\\ncreated above, and the results be visualized and analyzed in\\nreal-time.', 'Extracting the eye patch: After obtaining the location\\nof the extremities of an individual eye, some calculations\\nwere done to extract the Region of Interest (ROI) of the\\neye, which involved finding the x and y centers and applying\\na bit of padding to extract the said ROI. This is used for\\ndirectly running through the classification wrapper to get a\\nlabel value, which is further extended to measure the eye\\nblink rate in blinks per minute (as outlined in Algorithm 1).\\nThese results are finally rendered on the screen.', 'Note that the model parameters, the sequence of the layers,\\nthe reshaping and activation functions, etc., have been found\\nto be optimal for the given dataset. Hyperparameter tuning\\nhas been extensively done on the number of sequential', 'Fig. 1.\\nSample images from “Closed Eyes in the Wild” dataset: closed\\nand open eyes from (i) Eye patches only (non-RGB), and (ii) Face images\\nwarped.', 'helps reduce the risk of overfitting and allows the model to\\nachieve better accuracy with limited training data.', 'CNN and LSTM model: This is followed by sequentially\\narranging the layers to build the model as is typically done\\nfor deep learning architectures. The model first takes the\\nbase model (which itself is a CNN), followed by an average\\nPooling layer that reduces the dimensions of the extracted\\nfeatures while retaining all the important information. This\\nis followed by a Reshape layer that reshapes the output of\\nthe previous layer to a shape of 1x2048. Next, we have three\\nLSTM layers (with 128 cells each) that learn the temporal\\ndependencies in the features extracted by the previous layers.\\nAfter the LSTM layers, the output is passed through a Dense\\n(fully connected) layer with 2048 neurons and a ReLU\\nactivation function that learns a higher level representation\\nof the input. The next layer is a Dropout layer that randomly\\ndrops 30% of the neurons to reduce overfitting. Finally, the\\noutput is passed through a Dense layer with a single neuron\\nand a Sigmoid activation function to predict the probability\\nof the input image belonging to one of the two classes (eye\\nopen or eye closed) (Fig. 3). The model is compiled using the\\nAdam optimizer, Binary Cross-Entropy loss, and Accuracy\\nas the metric. The model is fitted on the training dataset for\\n15 epochs, and the predictions are then made over the testing\\ndataset.']\n",
      "\n",
      "\n",
      "V. USING OPENCV AND DLIB FOR EYE-BLINK\n",
      "DETECTION\n",
      "\n",
      "\n",
      "['Extending portability and classification function: The\\nweight parameters obtained from the trained model are then\\nexported for portability. This is followed by a wrapper\\nfunction that, given a localized eye patch within the usual\\ncolor space and normal image density, does the required', 'Algorithm 1\\n1 detector ← dlib.detector;\\n2 predictor ← dlib.predictor;\\n3 ratio = 1.3;\\n4 prev_label = -1;\\n5 for frame in load(VIDEO_STREAM or WEBCAM): do\\n6', 'grayscale ← convertGrayscale(frame);\\nfaces ← detector(grayscale);\\nfor face in faces: do', 'shape ← predictor(grayscale, face);\\nratio = 1.3;\\nxmi ← getXmi(shape);\\nxma ← getXma(shape);\\nxcent ← (xma + xmi) / 2;\\nycent ← getYcent(shape);\\nxmin, xmax, ymin, ymax ← getCoords(shape);\\nimage_patch ←', 'grayscale[ymin : ymax , xmin : xmax] / 255;', 'image_patch.resize(224,224);\\ncurr_label ← model.classify(image_patch);\\nif', 'curr_label == CLOSE && prev_label == OPEN:\\nthen', 'addBlinkEvent(datetime.now());', 'prev_label ← curr_label;', 'augmented_frame ← addModelFindings1(frame);\\nimshow(augmented_frame);', 'model layers2, dropout parameters, type of pooling layers,\\nand activation functions. The training time was reasonable\\naccording to modern standards, and the model size was\\ncompact (around 200 MB). Integrating the model with the\\nDlib library and pre-processing the images using OpenCV\\nhas been observed to perform sufficiently well in real-time.\\nIt is worth noting that since an average blink lasts between\\n0.1 and 0.4 seconds, an FPS of around 10 would suffice.\\nAdditionally, the actual duration of the blink does not matter\\nas long as the blink duration is not shorter than the frame\\ntime (0.1 milliseconds). However, currently the experimental\\nscript employed for the eye images model acquires live video\\ninput at a frame rate ranging from 8 to 10 FPS. Notably, the\\nentire process is executed solely on the CPU. The potential\\nfor performance enhancement is substantial if a GPU-based\\niteration is pursued. However, there might not be a need to\\ncapture such fast blinks because the current setup is enough\\nfor real-time monitoring.']\n",
      "\n",
      "\n",
      "VI. RESULTS\n",
      "\n",
      "\n",
      "['The combination of CNN and LSTM layers is effective for\\neye blink classification as it captures both spatial and tem-\\nporal information in input data. CNN layers automatically\\nextract important features from raw images for blink or non-\\nblink classification. Trained on large image datasets, CNN', '1addBlinkEvent and addModelFindings serve as placeholders for CVS', 'detection logic, explained in Future Work.', '2Except the CNN layers, since they are provided by the base Inception-v3', 'model and are frozen during training.', 'layers learn filters to identify features, generating a feature\\nmap indicating their presence in the image. However, blink\\nclassification also involves monitoring temporal changes.\\nLSTM layers, designed for modeling temporal dependencies,\\nprocess input frames sequentially to learn blink rate patterns\\nover time.', 'The model was first trained on individual eye patches so as\\nto make it focus solely on the region of the image containing\\nthe eye. This was originally intended to result in a more\\naccurate classification of eye blinks, but the fact that the\\nmodel may struggle to generalize to new images with differ-\\nent backgrounds or lighting conditions was also considered.\\nMoreover, according to the original hypothesis, training the\\nmodel on complete face images could allow the model to\\nlearn to recognize subtle changes in facial expression or head\\npose, which could be important in detecting eye blinks. This\\ncan improve the efficiency and speed of the model in some\\ninstances.', 'However, on running the latter model, we observed a slight\\ndrop in the speed of detecting eye blinks, as well as the\\naccuracy of measuring the frequency (Fig. 4). In principle,\\npassing complete facial images to the model makes more\\nsense since the layers would then also take into account the\\nlighting conditions, eyebrows, etc. while classifying. How-\\never, in practice, it is computationally inefficient and more\\nprohibitive than the former. It can be assumed that since the\\nfacial images dataset is smaller than the eye images dataset,\\nthe model finds it difficult to train itself in the presence\\nof outliers (due to lighting condition variations, etc.), and\\nincreasing the number of epochs results in overfitting, or\\nthat the model may not be as effective at capturing the subtle\\nchanges in the eye region that are indicative of an eye blink.']\n",
      "\n",
      "\n",
      "TABLE III\n",
      "PERFORMANCE OF THE FINAL MODEL (TRAINED OVER 15 EPOCHS).\n",
      "\n",
      "\n",
      "['Dataset\\nEye patches only (non-RGB)\\nFace images warped (RGB)', 'Train accuracy\\n99.4 %\\n96.7 %', 'Test accuracy\\n94.2 %\\n91.4 %', 'After building the model with the aforementioned ar-\\nchitecture, we first trained it on the individual non-RGB\\nimages of eye patches from the dataset (Fig. 5). We observed\\nthe training accuracy obtained after 15 epochs was\\nthat\\n99.4%, and the testing accuracy was 94.2% (Precision =\\n91.7% and Recall = 95.9%). The trained model worked\\nsignificantly well in real-time as well, in terms of speed and\\naccuracy. This was followed by running the model on face-\\npass images (RGB), giving a 96.7% training accuracy and\\na 91.4% testing accuracy (Precision = 84.6% and Recall =\\n93.2%). On increasing the number of epochs used to train the\\nmodel, a significant improvement was noticed in the training\\naccuracy, which was not the case with individual eye images.\\nHowever, testing accuracy did not change much. Note that\\nthe model was trained and evaluated using a single run\\nwithout employing n-fold validation, and that these metrics\\nindicate the ability of the model to classify static images', 'Fig. 2. Leveraging Transfer Learning: Inception-v3 base model (trained on ImageNet dataset) provides pre-trained CNN layers.', 'Fig. 3. Model architecture diagram.', 'as open or closed eyes. However, the final script used in\\nthe experiment, which takes a live stream video of the user\\nas input, also demonstrates notably high performance in\\naccurately assessing the blink rate.']\n",
      "\n",
      "\n",
      "VII. CONCLUSION\n",
      "\n",
      "\n",
      "['After training the model and importing it, the user’s live\\nvideo stream (captured via a webcam) is segmented into a\\nsequence of image inputs, classified independently to detect\\nblinks and consequently the blink rate (in blinks per minute).\\nThis script runs well in the background, enabling users to\\nidentify the initial stages of CVS and implement preven-\\ntive actions (such as eye exercise and acupressure ).\\nAdditionally, the experimental script may prompt reminders\\nfor the user to blink. A conclusion can then be drawn\\nby contrasting the user’s current blink frequency with the\\naverage blinking patterns. However, confirming the presence\\nof CVS is a much more onerous task and there do not', 'exist sufficient datasets and parameter studies that can help\\nin the correct diagnosis. Moreover, we recognize that long-\\nterm monitoring and evaluation of several other parameter\\nmeasurements are imperative to confirm its presence. As\\nsuch, future research endeavors must encompass extended\\nobservation periods and a broader array of metrics to achieve\\na more conclusive understanding of CVS and its potential\\nassociations with ocular health in the context of digital device\\nusage.']\n",
      "\n",
      "\n",
      "VIII. FUTURE WORK\n",
      "\n",
      "\n",
      "['To confirm CVS in users, it is crucial to gather data on\\nvarious eye conditions linked to it. Red eyes, for instance, are\\na common CVS symptom and a valuable indicator. Addition-\\nally, dry eyes, eye fatigue, and eye strain can suggest CVS.\\nCollecting this data can enable the creation of a predictive\\nmodel to alert users about CVS risk. Using eye-tracking\\ntechnology to monitor users’ eye movements when using']\n"
     ]
    }
   ],
   "source": [
    "for heading, content in parse(text):\n",
    "    print('\\n\\n'+heading+'\\n\\n')\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eye Movement Tracking for Computer Vision Syndrome using\\nDeep Learning Techniques']\n",
      "['']\n",
      "['1Department of Computer Science, BITS Pilani, Hyderabad Campus, India\\n2Department of Pharmacy, BITS Pilani, Hyderabad Campus, India\\n{f20200029, f20200042, f20200247, nirmalj, hota}@hyderabad.bits-pilani.ac.in']\n",
      "['drying of the eyes, eye strain, headaches, blurred vision, etc.,\\nNeural Network (CNN) to detect eye blinks, and monitoring\\nblink rates with a Long Short-Term Memory (LSTM) network.\\nrate and eye movement patterns have also been identified.\\nhas been trained on the Closed Eyes in the Wild (CEW) dataset.']\n",
      "['Computer Vision Syndrome (CVS) is a term that\\nand eye-related difficulties caused by the increased usage\\nas “digital eye strain”, causes problems such as dry eyes,\\nto monitor the blinking rate, which is a vital sign of CVS.\\nExisting research in eye blink detection for CVS has']\n",
      "['as it required setting distinct threshold values for each image,\\npotentially leading to noise-induced point detection when\\nusing low thresholds.\\nThese works underscore the need for\\nrobust illumination strategies, stable lighting conditions, and\\nimproved threshold determination methods to enhance the\\naccuracy and reliability of eye blink detection techniques for\\nCVS mitigation.']\n",
      "['Bennett et al.\\nmentation and gaze estimation in MR eye videos using a\\nHowever, their approach\\nmainly focused on static frame-by-frame training and did\\nIn another study, Jurczak et al.\\nresearch focused on a different data type and domain, and\\nthe application to CVS eye blink detection requires a tailored\\nand robust approach.']\n",
      "['Vishesh et al.\\nusing MobileNetV2-based CNN to assess drowsiness levels\\nin drivers.\\nAdditionally, Gomaa et al.\\nduring screen interaction, that often involves close-up images\\ndevices under various lighting conditions.\\nIn contrast, driver\\ndrowsiness detection necessitates real-time monitoring of\\nTechniques often involve more robust algorithms\\ndistinct goals and operational conditions.']\n",
      "['In light of these existing studies, our research introduces\\nconvolutional neural network (CNN) to extract eye features,\\nfollowed by a long short-term memory (LSTM) network to\\nmodel the temporal evolution of eye blinking in a continu-\\nThe general motivation behind using a CNN network for\\nin visual data and automatically extract features from raw']\n",
      "['images.\\nAdditionally, LSTM layers help model\\ntemporal\\ndependencies in sequential data and are commonly used\\nfor taking a continuous input stream (video frames) and\\nmonitoring the blink rate in real-time with accuracy and\\nrobustness.']\n",
      "['Our methodology involved applying the proposed algo-\\nrithm to a diverse dataset of images of individual faces and\\neye patches collected from various sources.\\nThe output of\\nour model provides frame-level classifications, labelling each\\nframe as either “eye open” or “eye closed”.\\nThis research\\nmakes the following key contributions:']\n",
      "['(i) Introduction of a dual-stage network that combines\\neye features extracted via a CNN with their dynamic rep-\\nresentation modeled by an LSTM.\\nThis approach is tailored\\nspecifically for analyzing live stream videos of digital device\\nusers.']\n",
      "['(ii) Thorough experimental evaluation of model parame-\\nters, such as LSTM cell units and layers, and deploying the\\nmodel using OpenCV and Dlib for real-time monitoring.']\n",
      "['(iii) A survey of the various studies on the impact of usage\\nof electronic devices on the blink rate and eye movement\\npatterns.']\n",
      "['The experimental results demonstrated the model’s robust-\\nness in detecting eye states, effectively handling considerable\\nvariations in visual quality and eye positions.']\n",
      "['Abnormalities in blinking causes the tear film to\\nVarious studies have investigated the blink rate and\\nthe duration between blinks.\\nthe average spontaneous blink rate ranges between 12 and\\n15 times per minute.\\nIn addition, the period between blinks\\nder relaxed conditions, a mean blink rate of up to 22 per']\n",
      "['In many studies , , the blink rate has been observed\\nto about 6 per minute while reading.\\nuse VDTs frequently complain of eye discomfort and fatigue.\\nAccording to a prior study , tired eyes was one of the\\n(40%), while 30% reported dry eyes and eye discomfort.']\n",
      "['The blink rate of dry eye subjects was significantly higher\\ncompared to the control group (according to a t-test with\\ngroups showed a significant decrease in blink rate during\\ntear break-up and blink behavior was complex and observed\\nincomplete blinks was linked to more symptoms.\\nblink score and symptoms, indicating that a higher blink rate']\n",
      "['Eyes in the Wild (CEW) .\\nEye patches\\nwere collected using a face detector and eye localization\\nsoftware based on the coarse face region and eye position.\\nand eye patches measuring 24x24 pixels were extracted from\\nthe eye position.\\nface images with background; Face images warped; and Eye']\n",
      "['Pre-processing: Firstly, we load the images separately as\\nthen shuffled and divided into features (image strings) and\\ntarget (binary labels), followed by test-train split (with test\\nsize = 0.2) and the required reshaping of images to 224x224.\\nThe target column is then converted into one-hot encoded']\n",
      "['Transfer learning: Next, we build the model.\\nInception-v3 as the base model (Fig. 2), which serves as\\ntheir weights are frozen during training).\\nmodel can learn to generalize better and hence requires\\nless training since the base model has already learned the\\nthe-art models for image classification).']\n",
      "['Parameter\\nBase model\\nPretraining dataset\\nReshape layer target shape\\nNumber of LSTM units\\nDepth of stacked LSTM\\nNumber of neurons in first dense layer\\nActivation function for first dense layer\\nDropout layer argument\\nActivation function for final dense (classification) layer']\n",
      "['Value\\nInception-v3\\nImageNet\\nReLU\\nSigmoid']\n",
      "['Parameter\\nModel optimizer\\nLoss function\\nMetric\\nNumber of epochs for training']\n",
      "['Value\\nAdam\\nBinary cross-entropy\\nAccuracy']\n",
      "['pre-processing (resizing to shape 224x224 and normalizing\\nthe image patch) before running it through the classifier and\\nreturning the processed output.']\n",
      "['Dlib landmark detection: Dlib is an open-source library\\nDlib’s facial landmark detection model\\nused this model for the localization of eye and face patches\\n(which is further used to identify the ROI for the model).\\nand applying a buffer, we get the location of individual eyes']\n",
      "['Extracting the eye patch: After obtaining the location\\nof the extremities of an individual eye, some calculations\\nwere done to extract the Region of Interest (ROI) of the\\na bit of padding to extract the said ROI.\\nlabel value, which is further extended to measure the eye']\n",
      "['Note that the model parameters, the sequence of the layers,\\nthe reshaping and activation functions, etc., have been found\\nto be optimal for the given dataset.\\nHyperparameter tuning\\nhas been extensively done on the number of sequential']\n",
      "['Fig. 1.\\nSample images from “Closed Eyes in the Wild” dataset: closed\\nand open eyes from (i) Eye patches only (non-RGB), and (ii) Face images\\nwarped.']\n",
      "['helps reduce the risk of overfitting and allows the model to\\nachieve better accuracy with limited training data.']\n",
      "['arranging the layers to build the model as is typically done\\nLSTM layers (with 128 cells each) that learn the temporal\\nAfter the LSTM layers, the output is passed through a Dense\\noutput is passed through a Dense layer with a single neuron\\nof the input image belonging to one of the two classes (eye']\n",
      "['Extending portability and classification function: The\\nweight parameters obtained from the trained model are then\\nexported for portability.\\nThis is followed by a wrapper\\nfunction that, given a localized eye patch within the usual\\ncolor space and normal image density, does the required']\n",
      "['Algorithm 1\\n1 detector ← dlib.detector;\\n2 predictor ← dlib.predictor;\\n3 ratio = 1.3;\\n4 prev_label = -1;\\n5 for frame in load(VIDEO_STREAM or WEBCAM): do']\n",
      "['grayscale ← convertGrayscale(frame);\\nfaces ← detector(grayscale);\\nfor face in faces: do']\n",
      "['shape ← predictor(grayscale, face);\\nratio = 1.3;\\nxmi ← getXmi(shape);\\nxma ← getXma(shape);\\nxcent ← (xma + xmi) / 2;\\nycent ← getYcent(shape);\\nxmin, xmax, ymin, ymax ← getCoords(shape);\\nimage_patch ←']\n",
      "['']\n",
      "['image_patch.resize(224,224);\\ncurr_label ← model.classify(image_patch);']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['augmented_frame ← addModelFindings1(frame);\\nimshow(augmented_frame);']\n",
      "['Integrating the model with the\\nhas been observed to perform sufficiently well in real-time.\\nas long as the blink duration is not shorter than the frame\\nscript employed for the eye images model acquires live video\\ncapture such fast blinks because the current setup is enough\\nfor real-time monitoring.']\n",
      "['The combination of CNN and LSTM layers is effective for\\neye blink classification as it captures both spatial and tem-\\nporal information in input data.\\nCNN layers automatically\\nextract important features from raw images for blink or non-\\nblink classification.\\nTrained on large image datasets, CNN']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['layers learn filters to identify features, generating a feature\\nmap indicating their presence in the image.\\nHowever, blink\\nclassification also involves monitoring temporal changes.\\nLSTM layers, designed for modeling temporal dependencies,\\nprocess input frames sequentially to learn blink rate patterns\\nover time.']\n",
      "['The model was first trained on individual eye patches so as\\nthe eye.\\naccurate classification of eye blinks, but the fact that the\\nmodel may struggle to generalize to new images with differ-\\nmodel on complete face images could allow the model to\\npose, which could be important in detecting eye blinks.']\n",
      "['However, on running the latter model, we observed a slight\\npassing complete facial images to the model makes more\\nlighting conditions, eyebrows, etc.\\nfacial images dataset is smaller than the eye images dataset,\\nof outliers (due to lighting condition variations, etc.), and\\nthat the model may not be as effective at capturing the subtle']\n",
      "['Dataset\\nEye patches only (non-RGB)\\nFace images warped (RGB)']\n",
      "['']\n",
      "['']\n",
      "['the training accuracy obtained after 15 epochs was\\nThe trained model worked\\npass images (RGB), giving a 96.7% training accuracy and\\na 91.4% testing accuracy (Precision = 84.6% and Recall =\\nmodel, a significant improvement was noticed in the training\\nthe model was trained and evaluated using a single run']\n",
      "['Fig. 2.\\nLeveraging Transfer Learning: Inception-v3 base model (trained on ImageNet dataset) provides pre-trained CNN layers.']\n",
      "['Fig. 3.\\nModel architecture diagram.']\n",
      "['as open or closed eyes.\\nHowever, the final script used in\\nthe experiment, which takes a live stream video of the user\\nas input, also demonstrates notably high performance in\\naccurately assessing the blink rate.']\n",
      "['This script runs well in the background, enabling users to\\nidentify the initial stages of CVS and implement preven-\\nfor the user to blink.\\nby contrasting the user’s current blink frequency with the\\naverage blinking patterns.\\nof CVS is a much more onerous task and there do not']\n",
      "['exist sufficient datasets and parameter studies that can help\\nin the correct diagnosis.\\nMoreover, we recognize that long-\\nterm monitoring and evaluation of several other parameter\\nmeasurements are imperative to confirm its presence.\\nsuch, future research endeavors must encompass extended\\nobservation periods and a broader array of metrics to achieve']\n",
      "['To confirm CVS in users, it is crucial to gather data on\\nvarious eye conditions linked to it.\\nRed eyes, for instance, are\\nally, dry eyes, eye fatigue, and eye strain can suggest CVS.\\nmodel to alert users about CVS risk.\\nUsing eye-tracking\\ntechnology to monitor users’ eye movements when using']\n"
     ]
    }
   ],
   "source": [
    "summary = \"\"\n",
    "for [heading, content] in parse(text):\n",
    "    summary += '\\n\\n'+heading+'\\n\\n'\n",
    "    for para in content:\n",
    "        predicted = textrank(corpus=para, ratio=0.2, words=50)\n",
    "        print(predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
